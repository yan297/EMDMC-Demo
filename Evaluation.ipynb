{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "ABC_Top3 on D and E",
   "id": "b6decffa8f7b69b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:57:16.745333Z",
     "start_time": "2025-08-03T23:54:58.583501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/ABC_Top3\"\n",
    "main_folder_path = \"datasets/D\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "c3d95e36088531d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/ABC_Top3\n",
      "\n",
      "Total Accuracy: 81.16%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:57:42.438975Z",
     "start_time": "2025-08-03T23:57:16.760273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/ABC_Top3\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "844d4d90b329c69e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/ABC_Top3\n",
      "\n",
      "Total Accuracy: 26.94%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ABC_Top4 on D and E",
   "id": "58298126ec5ef6f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:59:40.783362Z",
     "start_time": "2025-08-03T23:57:42.445877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/ABC_Top4\"\n",
    "main_folder_path = \"datasets/D\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "35ce4f09c1baaf59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/ABC_Top4\n",
      "\n",
      "Total Accuracy: 80.33%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:00:08.378664Z",
     "start_time": "2025-08-03T23:59:40.843144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/ABC_Top4\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "e31ac3bb1a172f10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/ABC_Top4\n",
      "\n",
      "Total Accuracy: 78.56%\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BCD_Top3 on A and E",
   "id": "29bc4e8b5b717d97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:02:22.943986Z",
     "start_time": "2025-08-04T00:00:08.432955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BCD_Top3\"\n",
    "main_folder_path = \"datasets/A\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "6711fd8cef283174",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BCD_Top3\n",
      "\n",
      "Total Accuracy: 68.84%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:02:48.994188Z",
     "start_time": "2025-08-04T00:02:22.999551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BCD_Top3\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "32fb375371e36697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BCD_Top3\n",
      "\n",
      "Total Accuracy: 12.60%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BCD_Top4 on A and E",
   "id": "23e4ea5d32848a4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:05:02.649448Z",
     "start_time": "2025-08-04T00:02:49.046205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BCD_Top4\"\n",
    "main_folder_path = \"datasets/A\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "fa12e40d77d02e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BCD_Top4\n",
      "\n",
      "Total Accuracy: 42.49%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:05:28.078831Z",
     "start_time": "2025-08-04T00:05:02.705455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BCD_Top4\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "713fba4470b30aef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BCD_Top4\n",
      "\n",
      "Total Accuracy: 24.58%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AD_Top3 on B and C",
   "id": "71eb84b79aa396a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:06:23.593053Z",
     "start_time": "2025-08-04T00:05:28.127549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AD_Top3\"\n",
    "main_folder_path = \"datasets/B\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "f7603eb3395f1239",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AD_Top3\n",
      "\n",
      "Total Accuracy: 63.35%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:35:15.548320Z",
     "start_time": "2025-08-04T00:06:23.642952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AD_Top3\"\n",
    "main_folder_path = \"datasets/C\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "4e0e78ebaf1320bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AD_Top3\n",
      "\n",
      "Total Accuracy: 65.12%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AD_Top4 on B and C",
   "id": "49a82ae6d1832afe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:36:10.901182Z",
     "start_time": "2025-08-04T00:35:15.612352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AD_Top4\"\n",
    "main_folder_path = \"datasets/B\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "e908b6bd213a7fc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AD_Top4\n",
      "\n",
      "Total Accuracy: 54.35%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:05:02.463022Z",
     "start_time": "2025-08-04T00:36:10.949939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AD_Top4\"\n",
    "main_folder_path = \"datasets/C\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "fe6f19db0eabd469",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AD_Top4\n",
      "\n",
      "Total Accuracy: 52.75%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AB_Top3 on D and E",
   "id": "634605449313fc81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:36:03.556405Z",
     "start_time": "2025-08-04T01:34:02.220977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AB_Top3\"\n",
    "main_folder_path = \"datasets/D\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "321537e2d3cdd92c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AB_Top3\n",
      "\n",
      "Total Accuracy: 67.32%\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:36:30.809992Z",
     "start_time": "2025-08-04T01:36:03.870130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AB_Top3\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "2bc076537c77883c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AB_Top3\n",
      "\n",
      "Total Accuracy: 21.53%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AB_Top4 on D and E",
   "id": "b4cf7fda7de41e6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:43:23.309865Z",
     "start_time": "2025-08-04T01:41:14.182720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AB_Top4\"\n",
    "main_folder_path = \"datasets/D\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "472f2193439d0e98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AB_Top4\n",
      "\n",
      "Total Accuracy: 51.65%\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:43:50.868716Z",
     "start_time": "2025-08-04T01:43:23.367183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/AB_Top4\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "df1e20a24b57ff4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/AB_Top4\n",
      "\n",
      "Total Accuracy: 21.99%\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BD_Top3 on A and C and E",
   "id": "572a1c5f938a5366"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T01:46:06.431741Z",
     "start_time": "2025-08-04T01:43:50.922530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BD_Top3\"\n",
    "main_folder_path = \"datasets/A\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "c1f9150c85a0a84c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BD_Top3\n",
      "\n",
      "Total Accuracy: 26.91%\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T02:15:04.193449Z",
     "start_time": "2025-08-04T01:46:06.492775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BD_Top3\"\n",
    "main_folder_path = \"datasets/C\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "755e3d326f0ca96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BD_Top3\n",
      "\n",
      "Total Accuracy: 66.77%\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BD_Top4 on A and C and E",
   "id": "392f6f325700839f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T02:17:17.387946Z",
     "start_time": "2025-08-04T02:15:04.249352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BD_Top4\"\n",
    "main_folder_path = \"datasets/A\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "e407f8227c16f8d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BD_Top4\n",
      "\n",
      "Total Accuracy: 12.18%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T02:46:05.616682Z",
     "start_time": "2025-08-04T02:17:17.449169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BD_Top4\"\n",
    "main_folder_path = \"datasets/C\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "d29cb9b451064a59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BD_Top4\n",
      "\n",
      "Total Accuracy: 27.18%\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T02:46:31.196448Z",
     "start_time": "2025-08-04T02:46:05.677799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Evaluation script for zero-shot classification using a finetuned MedCLIP model.\n",
    "\n",
    "Loads a finetuned MedCLIP model, processes breast cancer X-ray images from a test dataset,\n",
    "classifies them by comparing image and text embeddings, and computes overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from medclip import MedCLIPProcessor, MedCLIPModel, MedCLIPVisionModelViT\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from huggingface_hub and transformers\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.*\")\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "model_dir = \"models/BD_Top4\"\n",
    "main_folder_path = \"datasets/E\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ------------------- Get candidate labels -------------------\n",
    "candidate_labels = sorted([d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d))])\n",
    "print(\"Candidate labels:\", candidate_labels)\n",
    "\n",
    "# ------------------- Load model and processor -------------------\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(model_dir)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Generate text embeddings -------------------\n",
    "text_inputs = processor(text=candidate_labels, return_tensors=\"pt\", padding=True)\n",
    "for k, v in text_inputs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        text_inputs[k] = v.cuda()\n",
    "with torch.no_grad():\n",
    "    text_embeds = model.encode_text(input_ids=text_inputs[\"input_ids\"], attention_mask=text_inputs[\"attention_mask\"])\n",
    "\n",
    "# ------------------- Helper functions -------------------\n",
    "def get_image_paths(main_folder_path):\n",
    "    \"\"\"Returns all image paths and their true labels from the test dataset.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in candidate_labels:\n",
    "        subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(subfolder_path, filename))\n",
    "                labels.append(subfolder)\n",
    "    return image_paths, labels\n",
    "\n",
    "def batch_generator(items, batch_size):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "image_paths, true_labels = get_image_paths(main_folder_path)\n",
    "image_paths_and_labels = list(zip(image_paths, true_labels))\n",
    "\n",
    "for batch in batch_generator(image_paths_and_labels, BATCH_SIZE):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for image_path, label in batch:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                batch_images.append(img.copy())\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            continue\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    image_inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    for k, v in image_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_inputs[k] = v.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embeds = model.encode_image(pixel_values=image_inputs[\"pixel_values\"])\n",
    "\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits = logit_scale * image_embeds @ text_embeds.t()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "    predicted_labels = [candidate_labels[i] for i in predicted_indices]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(batch_labels)\n",
    "\n",
    "# ------------------- Compute accuracy -------------------\n",
    "correct = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == p)\n",
    "total = len(all_true_labels)\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nTotal Accuracy: {accuracy:.2f}%\")"
   ],
   "id": "95c7a866734f244f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate labels: ['The mammogram reveals the presence of benign findings characterized by one or more areas suggestive of the non-cancerous growth in the breast tissue', 'The mammogram reveals the presence of malignant findings characterized by one or more areas suggestive of the cancerous growth in the breast tissue', 'The mammogram shows healthy and normal breast tissue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/zyan297/anaconda3/envs/EMDMC-Demo/lib/python3.10/site-packages/medclip/modeling_medclip.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: models/BD_Top4\n",
      "\n",
      "Total Accuracy: 42.81%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73a0e415e9070a7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
